{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX7NqUokv5R9rH53x3FRfN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susuhlaingmyk26-tech/Colab-project/blob/main/unseen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_kQ_y8x2fOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Myanmar Text N-Gram Language Model (Bigram)\n",
        "# ===============================\n",
        "\n",
        "# 1Ô∏è‚É£ Basic libraries\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# 2Ô∏è‚É£ Corpus (Training Data)\n",
        "corpus = [\n",
        "    \"·Äô·Äº·Äî·Ä∫·Äô·Ä¨ ·Äû·ÄÑ·Ä∫·Äπ·ÄÅ·Äª·Ä¨ ·ÄÖ·Ä≠·Äê·Ä∫·Äù·ÄÑ·Ä∫·ÄÖ·Ä¨·Ä∏ ·ÄÖ·Äõ·Ä¨·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äê·Äö·Ä∫\",\n",
        "    \"·Äô·Äº·Äî·Ä∫·Äô·Ä¨ NLP ·ÄÄ·Ä≠·ÄØ ·Äú·Ä±·Ä∑·Äú·Ä¨·Äî·Ä±·Äê·Äö·Ä∫\",\n",
        "    \"NLP ·Äû·Ää·Ä∫ ·ÄÖ·Ä≠·Äê·Ä∫·Äù·ÄÑ·Ä∫·ÄÖ·Ä¨·Ä∏ ·ÄÖ·Äõ·Ä¨·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äê·Äö·Ä∫\",\n",
        "    \"·ÄÄ·Äª·ÄΩ·Äî·Ä∫·Äê·Ä±·Ä¨·Ä∫ ·Äô·Äº·Äî·Ä∫·Äô·Ä¨ ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ä≠·ÄØ·ÄÄ·Ä∫·Äê·Äö·Ä∫\"\n",
        "]\n",
        "\n",
        "# 3Ô∏è‚É£ Tokenization\n",
        "def tokenize(sentence):\n",
        "    tokens = sentence.strip().split()\n",
        "    return [\"<s>\"] + tokens + [\"</s>\"]\n",
        "\n",
        "tokenized_corpus = []\n",
        "for sent in corpus:\n",
        "    tokenized_corpus.extend(tokenize(sent))\n",
        "\n",
        "# 4Ô∏è‚É£ Build n-grams\n",
        "def build_ngrams(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "    return ngrams\n",
        "\n",
        "bigrams = build_ngrams(tokenized_corpus, 2)\n",
        "\n",
        "# 5Ô∏è‚É£ Count unigrams & bigrams\n",
        "unigram_counts = Counter(tokenized_corpus)\n",
        "bigram_counts = Counter(bigrams)\n",
        "V = len(unigram_counts)\n",
        "\n",
        "# 6Ô∏è‚É£ Bigram probability (No smoothing)\n",
        "def bigram_prob(w1, w2):\n",
        "    return bigram_counts[(w1, w2)] / unigram_counts[w1]\n",
        "\n",
        "# 7Ô∏è‚É£ Add-One (Laplace) smoothing\n",
        "def bigram_add_one(w1, w2):\n",
        "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[w1] + V)\n",
        "\n",
        "# 8Ô∏è‚É£ Sentence probability\n",
        "def sentence_probability(sentence, smoothing=\"addone\"):\n",
        "    tokens = tokenize(sentence)\n",
        "    prob = 1.0\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        w1, w2 = tokens[i], tokens[i+1]\n",
        "\n",
        "        if smoothing == \"addone\":\n",
        "            prob *= bigram_add_one(w1, w2)\n",
        "\n",
        "        elif smoothing == \"nosmooth\":\n",
        "            if bigram_counts[(w1, w2)] == 0:\n",
        "                return 0\n",
        "            prob *= bigram_prob(w1, w2)\n",
        "\n",
        "    return prob\n",
        "\n",
        "# 9Ô∏è‚É£ Perplexity\n",
        "def perplexity(sentence, smoothing=\"addone\"):\n",
        "    tokens = tokenize(sentence)\n",
        "    N = len(tokens) - 1\n",
        "    p = sentence_probability(sentence, smoothing)\n",
        "\n",
        "    if p == 0:\n",
        "        return float(\"inf\")\n",
        "\n",
        "    return pow(1/p, 1/N)\n",
        "\n",
        "# üîü Test sentence\n",
        "test_sentence = \"·Äô·Äº·Äî·Ä∫·Äô·Ä¨ NLP ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ä≠·ÄØ·ÄÄ·Ä∫·Äê·Äö·Ä∫\"\n",
        "\n",
        "print(\"Sentence Probability:\", sentence_probability(test_sentence))\n",
        "print(\"Perplexity:\", perplexity(test_sentence))\n",
        "\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Count of counts (for Good-Turing idea)\n",
        "count_of_counts = Counter(bigram_counts.values())\n",
        "print(\"Count of counts:\", count_of_counts)\n",
        "\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ Unseen bigram probability (Good-Turing simplified)\n",
        "N1 = count_of_counts[1]\n",
        "N = sum(bigram_counts.values())\n",
        "N0 = (len(unigram_counts) ** 2) - len(bigram_counts)\n",
        "\n",
        "p_unseen = N1 / (N * N0)\n",
        "print(\"Unseen bigram probability:\", p_unseen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLK-mK8L3WIJ",
        "outputId": "827e4ac9-4f4f-4576-bfb2-2b353c34120b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Probability: 7.849293563579277e-05\n",
            "Perplexity: 6.6226817481398275\n",
            "Count of counts: Counter({1: 14, 2: 3, 3: 1})\n",
            "Unseen bigram probability: 0.004830917874396135\n"
          ]
        }
      ]
    }
  ]
}